Experiment_name: kinetics_twin_attn

# feeder
feeder: feeders.feeder_kinetics.Feeder
train_feeder_args:
  data_path: data/kinetics-skeleton/train_data.npy
  label_path: data/kinetics-skeleton/train_label.pkl
  random_choose: True
  random_move: True
  window_size: 120
  debug: False

test_feeder_args:
  data_path: data/kinetics-skeleton/val_data.npy
  label_path: data/kinetics-skeleton/val_label.pkl

# model
model: model.twin_attention.Model
model_args:
  num_class: 400
  num_point: 18
  num_person: 2
  graph: graph.sign_27.Graph
  groups: 16
  block_size: 41
  graph_args:
    labeling_mode: 'spatial'
    graph: 'kinetics'
  use_grpe: False
  inner_dim: 64
  window_size: 120
  s_num_heads: 4
  t_num_heads: 4
  t_depths: [2, 2, 2, 2]
  wss: [6, 6, 6, 6]
  sr_ratios: [8, 4, 2, 1]
  depth: 4
  s_pos_emb: False
  drop_layers: 2

#optim
weight_decay: 0.0001
base_lr: 0.1
step: [20, 30, 40, 50]

# training
device: [0, 1]
# weights: save_models/wlasl_all_attn-187.pt
# start_epoch: 188
keep_rate: 0.9
only_train_epoch: 1
batch_size: 40
test_batch_size: 40
num_epoch: 50
nesterov: True

wandb: False
wandb_project: SLGTformer First Run
wandb_entity: irvl
wandb_name: Twin Attention, Kinetics, 24BS

num_worker: 4
save_interval: 5