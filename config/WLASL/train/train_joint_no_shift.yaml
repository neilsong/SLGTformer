Experiment_name: wlasl_all_attn_no_s_pos_emb

# feeder
feeder: feeders.feeder.Feeder
train_feeder_args:
  data_path: data/train_data_joint.npy
  label_path: data/train_label.pkl
  debug: False
  random_choose: True
  window_size: 120
  random_shift: True
  normalization: True
  random_mirror: True
  random_mirror_p: 0.5
  is_vector: False
  lap_pe: False

test_feeder_args:
  data_path: data/val_data_joint.npy
  label_path: data/val_label.pkl
  random_mirror: False
  normalization: True
  lap_pe: False

# model
model: model.twin_attention.Model
model_args:
  num_class: 2000
  num_point: 27
  num_person: 1
  graph: graph.sign_27.Graph
  groups: 16
  block_size: 41
  graph_args:
    labeling_mode: 'spatial'
  use_grpe: False
  inner_dim: 64
  window_size: 120
  s_num_heads: 4
  t_num_heads: 4
  t_depths: [2, 2, 2, 2]
  wss: [6, 6, 6, 6]
  sr_ratios: [8, 4, 2, 1]
  depth: 4
  s_pos_emb: False
  drop_layers: 2

#optim
weight_decay: 0.0001
base_lr: 0.1
step: [150, 200]

# training
device: [0]
# weights: save_models/wlasl_all_attn-187.pt
# start_epoch: 188
keep_rate: 0.9
only_train_epoch: 1
batch_size: 24
test_batch_size: 24
num_epoch: 250
nesterov: True
warm_up_epoch: 20

wandb: True
wandb_project: SLGTformer First Run
wandb_entity: irvl
wandb_name: Twin Attention, No Shift, 24BS

num_worker: 4
save_interval: 10