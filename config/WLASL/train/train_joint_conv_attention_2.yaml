Experiment_name: wlasl_all_attn

# feeder
feeder: feeders.feeder.Feeder
train_feeder_args:
  data_path: data/train_data_joint.npy
  label_path: data/train_label.pkl
  debug: False
  random_choose: True
  window_size: 120
  random_shift: True
  normalization: True
  random_mirror: True
  random_mirror_p: 0.5
  is_vector: False
  lap_pe: False

test_feeder_args:
  data_path: data/val_data_joint.npy
  label_path: data/val_label.pkl
  random_mirror: False
  normalization: True
  lap_pe: False

# model
model: model.conv_attention_2.Model
model_args:
  num_class: 2000
  num_point: 27
  num_person: 1
  graph: graph.sign_27.Graph
  groups: 16
  block_size: 41
  graph_args:
    labeling_mode: 'spatial'
  use_gpsa: False
  inner_dim: 64
  window_size: 120

#optim
weight_decay: 0.0001
base_lr: 0.01
step: [150, 200]

# training
device: [1]
# weights: save_models/wlasl_joint_attn2-48.pt
# start_epoch: 49
keep_rate: 0.9
only_train_epoch: 1
batch_size: 24
test_batch_size: 24
num_epoch: 250
nesterov: True
warm_up_epoch: 20

wandb: False
wandb_project: SLGTformer First Run
wandb_entity: irvl
wandb_name: All Attention, 24BS, Hi LR, SGD

num_worker: 4
save_interval: 10