Experiment_name: gps_attn

# feeder
feeder: feeders.feeder.Feeder
train_feeder_args:
  data_path: data/train_data_joint.npy
  label_path: data/train_label.pkl
  debug: False
  random_choose: True
  window_size: 120
  random_shift: True
  normalization: True
  random_mirror: True
  random_mirror_p: 0.5
  is_vector: False
  lap_pe: True

test_feeder_args:
  data_path: data/val_data_joint.npy
  label_path: data/val_label.pkl
  random_mirror: False
  normalization: True
  lap_pe: True
  random_choose: True
  window_size: 120

# model
model: model.attention.Model
model_args:
  num_class: 2000
  num_point: 27
  num_person: 1
  graph: graph.sign_27.Graph
  groups: 16
  block_size: 41
  num_layers: 3
  num_heads: 4
  dim_h: 52
  attn_dropout: 0.5
  dim_pe: 8
  enc_layers: 2
  enc_n_heads: 4
  enc_post_layers: 0
  max_freqs: 8
  graph_args:
    labeling_mode: 'spatial'


#optim
optimizer: AdamW
weight_decay: 1e-5
base_lr: 0.01
step: [150, 200]

# training
device: [0]
# weights: save_models/csl_joint-182.pt
# start_epoch: 183
keep_rate: 0.9
only_train_epoch: 1
batch_size: 12
test_batch_size: 12
num_epoch: 250
nesterov: True
warm_up_epoch: 20
wandb: True
wandb_project: SLGTformer First Run
wandb_entity: irvl
wandb_name: Stock GPS Attention in SLGCN Net Skeleton, 12BS

num_worker: 4