Experiment_name: ntu_twin_attn

# feeder
feeder: feeders.feeder_kinetics.Feeder
train_feeder_args:
  data_path: data/ntu/xsub/train_data.npy
  label_path: data/ntu/xsub/train_label.pkl
  random_choose: True
  random_move: True
  window_size: 120
  debug: False

test_feeder_args:
  data_path: data/ntu/xsub/val_data.npy
  label_path: data/ntu/xsub/val_label.pkl
  random_choose: True
  window_size: 120

# model
model: model.twin_attention.Model
model_args:
  num_class: 60
  num_point: 25
  num_person: 2
  graph: graph.sign_27.Graph
  groups: 16
  block_size: 41
  graph_args:
    labeling_mode: 'spatial'
    graph: 'ntu'
  inner_dim: 64
  window_size: 120
  s_num_heads: 4
  t_num_heads: 4
  t_depths: [2, 2, 2, 2]
  wss: [6, 6, 6, 6]
  sr_ratios: [8, 4, 2, 1]
  depth: 4
  s_pos_emb: False
  drop_layers: 2

#optim
weight_decay: 0.0001
base_lr: 0.1
step: [60, 80]

# training
device: [0, 1]
# weights: save_models/wlasl_all_attn-187.pt
# start_epoch: 188
keep_rate: 0.9
only_train_epoch: 1
batch_size: 30
test_batch_size: 30
num_epoch: 100
nesterov: True
warm_up_epoch: 8

wandb: True
wandb_project: SLGTformer First Run
wandb_entity: irvl
wandb_name: Twin Attention, NTU, 30BS

num_worker: 4
save_interval: 10